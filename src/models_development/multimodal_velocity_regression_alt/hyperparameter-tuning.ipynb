{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "\n",
    "from ray import train as ray_train\n",
    "from ray import tune\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import params.learning\n",
    "from params.learning import LEARNING\n",
    "from dataset import get_dataloader\n",
    "from model import ResNet18Velocity_Regression_Alt\n",
    "from train import train\n",
    "from validate import validate\n",
    "from test import test\n",
    "from result import parameters_table, generate_log\n",
    "\n",
    "from custom_transforms import (\n",
    "    Cutout,\n",
    "    Shadowcasting,\n",
    ")\n",
    "\n",
    "from params import PROJECT_PATH\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ablation range of data augmentation\n",
    "\n",
    "image_augmentation_transforms = [\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.9, contrast=0.3, saturation=1, hue=0.1\n",
    "    ),\n",
    "    transforms.GaussianBlur(3),\n",
    "    transforms.GaussianBlur(7),\n",
    "    transforms.RandomSolarize(0.5),\n",
    "    Cutout(),\n",
    "    Shadowcasting(),\n",
    "]\n",
    "augmentation_transforms = [\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomResizedCrop(\n",
    "        params.learning.IMAGE_SHAPE,\n",
    "        scale=(0.2, 1.0),\n",
    "        ratio=(3, 3),\n",
    "        antialias=True,\n",
    "    ),\n",
    "    transforms.Normalize(\n",
    "        Tensor([0.4333, 0.4610, 0.4413, 0.0926, 0.4989, 0.5924, 0.8239]),\n",
    "        Tensor([0.2223, 0.2146, 0.2154, 0.0817, 0.2162, 0.2504, 0.1614]),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer hyperparameters\n",
    "optimizers = [\n",
    "    optim.Adam,\n",
    "    # optim.SGD,  # Clearly less performant than Adam on previous runs\n",
    "    # optim.RMSprop,\n",
    "    # optim.AdamW,\n",
    "    # optim.Adamax,\n",
    "    # optim.ASGD,\n",
    "    # optim.LBFGS,\n",
    "]\n",
    "learning_rates = [1e-5, 1e-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_augmentation_search_space = {\n",
    "    f\"image_augmentation_{i}\": tune.choice([True, False])\n",
    "    for i, aug in enumerate(image_augmentation_transforms)\n",
    "}\n",
    "\n",
    "augmentation_search_space = {\n",
    "    f\"augmentation_{i}\": tune.choice([True, False])\n",
    "    for i, aug in enumerate(augmentation_transforms)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"nb_epochs\": 200,\n",
    "    \"learning_rate\": tune.loguniform(*learning_rates),\n",
    "    \"optimizer\": tune.choice(optimizers),\n",
    "    **image_augmentation_search_space,\n",
    "    **augmentation_search_space,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black-box optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiple_choice(\n",
    "    config: dict, prefix: str, choices: []\n",
    ") -> transforms.Compose:\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            aug\n",
    "            for i, aug in enumerate(choices)\n",
    "            if f\"{prefix}_{i}\" in config and config[f\"{prefix}_{i}\"]\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_regression = nn.MSELoss()\n",
    "\n",
    "\n",
    "def trial(config):\n",
    "    image_augmentation_transform = get_multiple_choice(\n",
    "        config, \"image_augmentation\", image_augmentation_transforms\n",
    "    )\n",
    "    augmentation_transform = get_multiple_choice(\n",
    "        config, \"augmentation\", augmentation_transforms\n",
    "    )\n",
    "\n",
    "    model = ResNet18Velocity_Regression_Alt().to(device)\n",
    "    train_loader, val_loader, _ = get_dataloader(\n",
    "        params.learning.DATASET,\n",
    "        image_augmentation_transform=image_augmentation_transform,\n",
    "        augmentation_transform=augmentation_transform,\n",
    "        multimodal_transform=transforms.Resize(\n",
    "            params.learning.IMAGE_SHAPE, antialias=True\n",
    "        ),\n",
    "        batch_size=params.learning.LEARNING[\"batch_size\"],\n",
    "    )\n",
    "    optimizer = config[\"optimizer\"](\n",
    "        model.parameters(),\n",
    "        lr=config[\"learning_rate\"],\n",
    "    )\n",
    "\n",
    "    scheduler = optim.lr_scheduler.LinearLR(\n",
    "        optimizer=optimizer, total_iters=config[\"nb_epochs\"]\n",
    "    )\n",
    "\n",
    "    for epoch in range(config[\"nb_epochs\"]):\n",
    "        train_regression_loss = train(\n",
    "            model,\n",
    "            device,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            criterion_regression,\n",
    "            epoch,\n",
    "        )\n",
    "\n",
    "        val_regression_loss = validate(\n",
    "            model, device, val_loader, criterion_regression, epoch\n",
    "        )\n",
    "\n",
    "        ray_train.report(\n",
    "            {\n",
    "                \"train_loss\": train_regression_loss,\n",
    "                \"val_loss\": val_regression_loss,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(trial pid=507369)\u001b[0m   0%|          | 0/30 [00:00<?, ?batch/s]\n",
      "\u001b[2m\u001b[36m(trial pid=507369)\u001b[0m   0%|          | 0/30 [00:00<?, ?batch/s]\n",
      "\u001b[2m\u001b[36m(trial pid=507369)\u001b[0m   0%|          | 0/30 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 12:11:01,173\tWARNING tune.py:194 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2023-10-18 12:11:11,189\tINFO tune.py:1143 -- Total run time: 113.29 seconds (103.26 seconds for the tuning loop).\n",
      "2023-10-18 12:11:11,190\tWARNING tune.py:1158 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"/home/g_thomas/ray_results/trial_2023-10-18_12-09-17\", trainable=...)\n",
      "2023-10-18 12:11:11,195\tWARNING experiment_analysis.py:205 -- Failed to fetch metrics for 1 trial(s):\n",
      "- trial_0bfded91: FileNotFoundError('Could not fetch metrics for trial_0bfded91: both result.json and progress.csv were not found at /home/g_thomas/ray_results/trial_2023-10-18_12-09-17/trial_0bfded91_2_augmentation_0=True,augmentation_1=True,augmentation_2=False,augmentation_3=False,batch_size=134.0042,image_augme_2023-10-18_12-09-22')\n"
     ]
    }
   ],
   "source": [
    "algo = OptunaSearch()\n",
    "gpu_trial = tune.with_resources(trial, {\"cpu\": 12, \"gpu\": 1})\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    gpu_trial,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        search_alg=algo,\n",
    "        num_samples=100,\n",
    "    ),\n",
    "    param_space=search_space,\n",
    ")\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'nb_epochs': 200, 'batch_size': 64, 'learning_rate': 0.0009467398972249038, 'optimizer': <class 'torch.optim.adam.Adam'>, 'image_augmentation': 16, 'augmentation': 2}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparameters found were: \", results.get_best_result().config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(\n",
      "  metrics={'iterations': 21, 'train_loss': 0.6919330770986668, 'val_loss': 0.620546210805575},\n",
      "  path='/home/g_thomas/ray_results/trial_2023-10-16_17-55-44/trial_5f204acb_48_augmentation=2,batch_size=64,image_augmentation=16,learning_rate=0.0009,nb_epochs=200,optimizer=ref_ph_7bdb82fb_2023-10-17_06-13-13',\n",
      "  filesystem='local',\n",
      "  checkpoint=None\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(results.get_best_result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['iterations', 'train_loss', 'val_loss', 'timestamp', 'done',\n",
       "       'training_iteration', 'trial_id', 'date', 'time_this_iter_s',\n",
       "       'time_total_s', 'pid', 'hostname', 'node_ip', 'time_since_restore',\n",
       "       'iterations_since_restore', 'checkpoint_dir_name', 'config/nb_epochs',\n",
       "       'config/batch_size', 'config/learning_rate', 'config/optimizer',\n",
       "       'config/image_augmentation', 'config/augmentation', 'logdir'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = results.get_dataframe()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/g_thomas/ray_results/trial_2023-10-16_17-55-44'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-10-18 11:15:58,487 E 35227 35227] (raylet) node_manager.cc:3007: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 16de75f6b059170fc6343450c045b4dc49c26bd59454d217191e897a, IP: 147.250.35.113) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 147.250.35.113`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "results.experiment_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global-python",
   "language": "python",
   "name": "global-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
